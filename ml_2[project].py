# -*- coding: utf-8 -*-
"""ML-2[project].ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15RMOSOYIB5yPo-vyrVlLt_pv7oFjN-P_

Importing Various Libraries
"""

# Data Collection, Data Cleaning & Data Manipulation
import numpy as np
import pandas as pd
from sklearn import datasets
from google.colab import drive

# Data Visualization
import matplotlib

# Apple Watch and fitbit data processing and predect
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(font_scale = 1)

# Data Transformation
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from scipy import stats

# Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import FunctionTransformer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import make_pipeline
from sklearn.pipeline import Pipeline
from sklearn.compose import make_column_selector, make_column_transformer
from sklearn import set_config
set_config(display='diagram')

# Models Building
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler

# Classification Problems
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn. ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from sklearn import metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score

# Regression Problems
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression, LogisticRegression, SGDRegressor
from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, AdaBoostRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Hyperparameter Tuning
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
import warnings
warnings.filterwarnings('ignore')
from sklearn.exceptions import ConvergenceWarning

# Unsupervised Learning: Clustering
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, Birch, MeanShift, SpectralClustering
from sklearn.metrics import adjusted_rand_score

"""##Loading Data"""

data = pd.read_csv('/content/aw_fb_data.csv')

data

data.shape

df = pd.DataFrame(data)
column_to_remove = 'device'
df = df.drop(column_to_remove, axis=1)
df

"""##Data Visualization"""

import plotly.graph_objects as go
labels = ['Lying','Running 7 METs','Running 5 METs','Running 3 METs', 'Sitting', 'Self Pace walk']
values = train_set['activity'].value_counts()
colors = ['red', 'royalblue','green','yellow','pink','grey']
fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.5)])
fig.update_traces(hoverinfo='label+value',textfont_size=15,marker=dict(colors=colors))
fig.update_layout(annotations=[dict(text='6 types of Activity',
                                    x=0.50, y=0.5, font_size=15,
                                    showarrow=False)])
fig.show()

"""## Train - Test Split"""

from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(df, test_size = 0.2, random_state = 42)

train_set

test_set

"""## Mapping strings to int"""

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()


x_train = train_set.copy()
y_train = x_train.pop("activity")
y_train = label_encoder.fit_transform(y_train)
y_train

label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))

print("Label Mapping:")
for label, numeric_value in label_mapping.items():
    print(f"{label}: {numeric_value}")

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

y_train = label_encoder.fit_transform(y_train)

x_test = test_set.copy()
y_test = x_test.pop("activity")
y_test = label_encoder.fit_transform(y_test)

y_test

"""##Decision Tree"""

# Train a DecisionTreeClassifier on the training data
model = DecisionTreeClassifier()
model.fit(x_train, y_train)

# Make predictions on the test data
y_pred = model.predict(x_test)

from sklearn.metrics import classification_report
report = classification_report(y_test, y_pred)
print(f"Classification Report:\n{report}")

from sklearn.metrics import roc_curve
pred_prob = model.predict_proba(x_test)
y_pred_lr= model.predict_proba(x_test)
random_probs = [0 for i in range(len(y_test))]
fpr = {}
tpr = {}
thresh ={}
n_class = 6
for i in range(n_class):
  fpr[i], tpr[i], thresh[i] = roc_curve(y_test, y_pred_lr[:,i], pos_label=i)

colors = ['orange', 'green', 'blue', 'red', 'purple', 'brown']
for i in range(n_class):
  plt.plot(fpr[i], tpr[i], linestyle='--', color=colors[i], label='Class {}'.format(i))
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='best')
plt.show()

# Evaluate the performance of the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')
mse = mean_squared_error(y_test,y_pred)
print(f"Accuracy Decision Tree Classifier: {accuracy:.2f}")
print(f"Precision Decision Tree Classifier: {precision:.2f}")
print(f"Recall Decision Tree Classifier: {recall:.2f}")
print(f"F1 score Decision Tree Classifier: {f1:.2f}")
print(f"Mean square error Decision Tree Classifier:{mse:.2f}")
cm = confusion_matrix(y_test, y_pred)

# Creating a dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.
cm_df = pd.DataFrame(cm,
                     index = ['Lying','Running 3 METs','Running 5 METs','Running 7 METs','Self Pace walk','Sitting'],
                     columns = ['Lying','Running 3 METs','Running 5 METs','Running 7 METs','Self Pace walk','Sitting'])
# Plotting the confusion matrix
plt.figure(figsize=(10, 10))
sns.heatmap(cm_df /np.sum(cm_df,axis = 0), annot=True, fmt='.2%', cmap='Blues')
plt.title('Confusion Matrix:Decision Tree')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()

"""##KNN Model"""

# Train the k-NN classifier
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(x_train, y_train)

# Make predictions on the test set
y_pred = knn.predict(x_test)
# Evaluate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')
mse = mean_squared_error(y_test,y_pred)
print(f"Accuracy kNN classifier: {accuracy:.2f}")
print(f"Precision kNN classifier: {precision:.2f}")
print(f"Recall kNN classifier: {recall:.2f}")
print(f"F1 score kNN classifier: {f1:.2f}")
print(f"Mean square error kNN Classifier:{mse:.2f}")

from sklearn.metrics import classification_report
report = classification_report(y_test, y_pred)
print(f"Classification Report:\n{report}")

from sklearn.metrics import roc_curve
pred_prob = model.predict_proba(x_test)
y_pred_lr= model.predict_proba(x_test)
random_probs = [0 for i in range(len(y_test))]
fpr = {}
tpr = {}
thresh ={}
n_class = 6
for i in range(n_class):
  fpr[i], tpr[i], thresh[i] = roc_curve(y_test, y_pred_lr[:,i], pos_label=i)

colors = ['orange', 'green', 'blue', 'red', 'purple', 'brown']
for i in range(n_class):
  plt.plot(fpr[i], tpr[i], linestyle='--', color=colors[i], label='Class {}'.format(i))
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='best')
plt.show()

# confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Creating a dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.
cm_df = pd.DataFrame(cm,
                     index = ['Lying','Running 3 METs','Running 5 METs','Running 7 METs','Self Pace walk','Sitting' ],
                     columns = ['Lying','Running 3 METs','Running 5 METs','Running 7 METs','Self Pace walk','Sitting' ])
# Plotting the confusion matrix
plt.figure(figsize=(10, 10))
sns.heatmap(cm_df /np.sum(cm_df,axis = 0), annot=True, fmt='.2%', cmap='Blues')
plt.title('Confusion Matrix: kNN')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()

"""##Logistic Regression"""

# Train a logistic regression model on the training set
model = LogisticRegression(max_iter=10000, verbose = True)
model.fit(x_train, y_train)

y_pred = model.predict(x_test)

# Evaluate the performance of the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')
mse = mean_squared_error(y_test,y_pred)
print(f"Accuracy LogisticRegression Classifier: {accuracy:.2f}")
print(f"Precision LogisticRegression Classifier: {precision:.2f}")
print(f"Recall LogisticRegression Classifier: {recall:.2f}")
print(f"F1 score LogisticRegression Classifier: {f1:.2f}")
print(f"Mean square error LogisticRegression Classifier:{mse:.2f}")

from sklearn.metrics import classification_report
report = classification_report(y_test, y_pred)
print(f"Classification Report:\n{report}")

from sklearn.metrics import roc_curve
pred_prob = model.predict_proba(x_test)
y_pred_lr= model.predict_proba(x_test)
random_probs = [0 for i in range(len(y_test))]
fpr = {}
tpr = {}
thresh ={}
n_class = 6
for i in range(n_class):
  fpr[i], tpr[i], thresh[i] = roc_curve(y_test, y_pred_lr[:,i], pos_label=i)

colors = ['orange', 'green', 'blue', 'red', 'purple', 'brown']
for i in range(n_class):
  plt.plot(fpr[i], tpr[i], linestyle='--', color=colors[i], label='Class {}'.format(i))
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='best')
plt.show()

cm = confusion_matrix(y_test, y_pred)
# Creating a dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.
cm_df = pd.DataFrame(cm,
                     index = ['Lying','Running 3 METs','Running 5 METs','Running 7 METs','Self Pace walk','Sitting' ],
                     columns = [ 'Lying','Running 3 METs','Running 5 METs','Running 7 METs','Self Pace walk','Sitting'])
# Plotting the confusion matrix
plt.figure(figsize=(10, 10))
sns.heatmap(cm_df /np.sum(cm_df,axis = 0), annot=True, fmt='.2%', cmap='Blues')
plt.title('Confusion Matrix: LogisticRegression')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()

"""##Gaussian Naive Bayes"""

# Train the Naive Bayes classifier
nb = GaussianNB()
nb.fit(x_train, y_train)

# Make predictions on the test set
y_pred = nb.predict(x_test)

# Evaluate the performance of the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')
mse = mean_squared_error(y_test,y_pred)
print(f"Accuracy Naive Bayes Classifier: {accuracy:.2f}")
print(f"Precision Naive Bayes Classifier: {precision:.2f}")
print(f"Recall Naive Bayes Classifier: {recall:.2f}")
print(f"F1 score Naive Bayes Classifier: {f1:.2f}")
print(f"Mean square error Naive Bayes Classifier:{mse:.2f}")

from sklearn.metrics import classification_report
report = classification_report(y_test, y_pred)
print(f"Classification Report:\n{report}")

from sklearn.metrics import roc_curve
pred_prob = nb.predict_proba(x_test)
y_pred_lr= nb.predict_proba(x_test)
random_probs = [0 for i in range(len(y_test))]
fpr = {}
tpr = {}
thresh ={}
n_class = 6
for i in range(n_class):
  fpr[i], tpr[i], thresh[i] = roc_curve(y_test, y_pred_lr[:,i], pos_label=i)

colors = ['orange', 'green', 'blue', 'red', 'purple', 'brown']
for i in range(n_class):
  plt.plot(fpr[i], tpr[i], linestyle='--', color=colors[i], label='Class {}'.format(i))
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='best')
plt.show()

cm = confusion_matrix(y_test, y_pred)

# Creating a dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.
cm_df = pd.DataFrame(cm,
                     index = [ 'Lying','Running 3 METs','Running 5 METs','Running 7 METs','Self Pace walk','Sitting'],
                     columns = [ 'Lying','Running 3 METs','Running 5 METs','Running 7 METs','Self Pace walk','Sitting'])
# Plotting the confusion matrix
plt.figure(figsize=(10, 10))
sns.heatmap(cm_df /np.sum(cm_df,axis = 0), annot=True, fmt='.2%', cmap='Blues')
plt.title('Confusion Matrix: Gaussian Naive Bayes')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()

"""##Random Forest"""

from sklearn.ensemble import RandomForestClassifier

# Train the Random Forest classifier
rfc = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42, verbose= True)
rfc.fit(x_train, y_train)

# Make predictions on the test set
y_pred = rfc.predict(x_test)

# Evaluate the performance of the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')
mse = mean_squared_error(y_test,y_pred)
print(f"Accuracy Random Forest Classifier: {accuracy:.2f}")
print(f"Precision Random Forest Classifier: {precision:.2f}")
print(f"Recall Random Forest Classifier: {recall:.2f}")
print(f"F1 score Random Forest Classifier: {f1:.2f}")
print(f"Mean square error Random Forest Classifier:{mse:.2f}")

from sklearn.metrics import classification_report
report = classification_report(y_test, y_pred)
print(f"Classification Report:\n{report}")

from sklearn.metrics import roc_curve
pred_prob = rfc.predict_proba(x_test)
y_pred_lr= rfc.predict_proba(x_test)
random_probs = [0 for i in range(len(y_test))]
fpr = {}
tpr = {}
thresh ={}
n_class = 6
for i in range(n_class):
  fpr[i], tpr[i], thresh[i] = roc_curve(y_test, y_pred_lr[:,i], pos_label=i)

colors = ['orange', 'green', 'blue', 'red', 'purple', 'brown']
for i in range(n_class):
  plt.plot(fpr[i], tpr[i], linestyle='--', color=colors[i], label='Class {}'.format(i))
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='best')
plt.show()

cm = confusion_matrix(y_test, y_pred)

# Creating a dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.
cm_df = pd.DataFrame(cm,
                     index = ['Lying','Running 3 METs','Running 5 METs','Running 7 METs','Self Pace walk','Sitting' ],
                     columns = [ 'Lying','Running 3 METs','Running 5 METs','Running 7 METs','Self Pace walk','Sitting'])
# Plotting the confusion matrix
plt.figure(figsize=(10, 10))
sns.heatmap(cm_df /np.sum(cm_df,axis = 0), annot=True, fmt='.2%', cmap='Blues')
plt.title('Confusion Matrix: Random Forest')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()

"""##SVM"""

from sklearn.svm import SVC

# Train the SVM classifier
svm = SVC(kernel='rbf', C=1, gamma='scale', random_state=42, verbose=True, probability =  True)
svm.fit(x_train, y_train)

# Make predictions on the test set
y_pred = svm.predict(x_test)

# Evaluate the performance of the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')
mse = mean_squared_error(y_test,y_pred)
print(f"Accuracy SVClassifier: {accuracy:.2f}")
print(f"Precision SVClassifier: {precision:.2f}")
print(f"Recall SVClassifier: {recall:.2f}")
print(f"F1 score SVClassifier: {f1:.2f}")
print(f"Mean square error SVClassifier:{mse:.2f}")

from sklearn.metrics import classification_report
report = classification_report(y_test, y_pred)
print(f"Classification Report:\n{report}")

from sklearn.metrics import roc_curve
pred_prob = svm.predict_proba(x_test)
y_pred_lr= svm.predict_proba(x_test)
random_probs = [0 for i in range(len(y_test))]
fpr = {}
tpr = {}
thresh ={}
n_class = 6
for i in range(n_class):
  fpr[i], tpr[i], thresh[i] = roc_curve(y_test, y_pred_lr[:,i], pos_label=i)

colors = ['orange', 'green', 'blue', 'red', 'purple', 'brown']
for i in range(n_class):
  plt.plot(fpr[i], tpr[i], linestyle='--', color=colors[i], label='Class {}'.format(i))
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='best')
plt.show()

cm = confusion_matrix(y_test, y_pred)

# Creating a dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.
cm_df = pd.DataFrame(cm,
                     index = [ 'Lying','Running 3 METs','Running 5 METs','Running 7 METs','Self Pace walk','Sitting'],
                     columns = ['Lying','Running 3 METs','Running 5 METs','Running 7 METs','Self Pace walk','Sitting' ])
# Plotting the confusion matrix
plt.figure(figsize=(10, 10))
sns.heatmap(cm_df /np.sum(cm_df,axis = 0), annot=True, fmt='.2%', cmap='Blues')
plt.title('Confusion Matrix: SVM')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()

"""## AUC"""

from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_auc_score
n_classes = 6
average_auc = 0.0
y_test_binarized = label_binarize(y_test, classes=[0, 1, 2, 3, 4, 5])
for i in range(n_classes):
    average_auc += roc_auc_score(y_test_binarized[:, i], pred_prob[:, i])

average_auc /= n_classes

print("Average AUC:", average_auc)

"""##Precision Recall"""

from sklearn.metrics import precision_recall_curve
precision = dict()
recall = dict()

for i in range(n_class):
    precision[i], recall[i], _ = precision_recall_curve(y_test_binarized[:, i], pred_prob[:, i])

def print_recalls_precision(recall, precision, title):
    plt.figure(figsize=(6, 6))
    for i in range(n_class):
        plt.plot(recall[i], precision[i], linestyle='--', color=colors[i], label='Class {}'.format(i))
    plt.xlabel("Recall", fontsize=16)
    plt.ylabel("Precision", fontsize=16)
    plt.title("Precision vs Recall plot - {0}".format(title), fontsize=16)
    plt.axis([0, 1, 0, 1])
    plt.legend(loc="best")
    plt.show()

print_recalls_precision(recall, precision, "DecisionTreeClassifier")

"""##Gradient boosting"""

from sklearn.ensemble import GradientBoostingClassifier

gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
gb_classifier.fit(x_train, y_train)

# Make predictions on the test set
y_pred = gb_classifier.predict(x_test)
y_prob = gb_classifier.predict_proba(x_test)[:, 1]

# Evaluate the performance of the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')
mse = mean_squared_error(y_test,y_pred)
print(f"Accuracy GradientBoostingClassifier: {accuracy:.2f}")
print(f"Precision  GradientBoostingClassifier: {precision:.2f}")
print(f"Recall GradientBoostingClassifier : {recall:.2f}")
print(f"F1 score GradientBoostingClassifier : {f1:.2f}")
print(f"Mean square error GradientBoostingClassifier :{mse:.2f}")

from sklearn.metrics import classification_report
report = classification_report(y_test, y_pred)
print(f"Classification Report:\n{report}")

from sklearn.metrics import roc_curve
pred_prob = gb_classifier.predict_proba(x_test)
y_pred_lr=  gb_classifier.predict_proba(x_test)
random_probs = [0 for i in range(len(y_test))]
fpr = {}
tpr = {}
thresh ={}
n_class = 6
for i in range(n_class):
  fpr[i], tpr[i], thresh[i] = roc_curve(y_test, y_pred_lr[:,i], pos_label=i)

colors = ['orange', 'green', 'blue', 'red', 'purple', 'brown']
for i in range(n_class):
  plt.plot(fpr[i], tpr[i], linestyle='--', color=colors[i], label='Class {}'.format(i))
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='best')
plt.show()

cm = confusion_matrix(y_test, y_pred)

# Creating a dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.
cm_df = pd.DataFrame(cm,
                     index = [ 'Lying','Running 3 METs','Running 5 METs','Running 7 METs','Self Pace walk','Sitting'],
                     columns = ['Lying','Running 3 METs','Running 5 METs','Running 7 METs','Self Pace walk','Sitting' ])
# Plotting the confusion matrix
plt.figure(figsize=(10, 10))
sns.heatmap(cm_df /np.sum(cm_df,axis = 0), annot=True, fmt='.2%', cmap='Blues')
plt.title('Confusion Matrix: Gradient Boosting')
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.show()